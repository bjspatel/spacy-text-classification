{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_lg\n",
    "from  spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "punctuations = string.punctuation.replace(\"#\", \"\")\n",
    "parser = English()\n",
    "nlp = en_core_web_lg.load()\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    # Lower case all words and strip white spaces\n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "    # Remove all stop words and punctuations\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'late logan mark marktag</mark daily #ad cloud analytics thanks'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Token matchers\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # Create matcher to detect urls\n",
    "    pattern = [{ \"LIKE_URL\": True }]\n",
    "    matcher.add(\"UrlDetection\", None, pattern)\n",
    "    \n",
    "    # Create matcher to detect ...\n",
    "    pattern = [{ \"TEXT\": \"...\" }]\n",
    "    matcher.add(\"MoreDotsDetection\", None, pattern)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        sentence = sentence.replace(span.text, '')\n",
    "    \n",
    "    # Tokenize sentence and join\n",
    "    sentence = ' '.join(str(token) for token in spacy_tokenizer(sentence))\n",
    "    \n",
    "    # Remove twitter handles\n",
    "    sentence = remove_pattern(sentence, \"@[\\w]*\")\n",
    "    \n",
    "    sentence = '#'.join([phrase for phrase in [e[1:] for e in (' ' + sentence).split(\"#\")]])\n",
    "\n",
    "    # remove words with length less than 3 and not #\n",
    "    sentence = ' '.join([word for word in sentence.split() if word[0] == '#' or len(word)>3])\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "clean_sentence(\"The latest Logan's <mark>marktag</mark> DX Daily #AD (Cloud, AI/ML, Analytics & IoT)! https://t.co/Ac3cKz73Gx Thanks to @JD_Corporate... https://t.co/OOyzLIPxA2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29871\n"
     ]
    }
   ],
   "source": [
    "# DON'T RUN THIS\n",
    "# new_data = { 'results': [] }\n",
    "\n",
    "# with open('new_posts.txt', encoding=\"utf-8\") as json_file:\n",
    "#     new_data = json.load(json_file)\n",
    "\n",
    "# with open('posts.txt', encoding=\"utf-8\") as read_file:\n",
    "#     data = json.load(read_file)\n",
    "#     new_data['results'] = new_data['results'] + data['results']\n",
    "    \n",
    "#     print(len(new_data['results']))\n",
    "#     with open('new_posts.txt', 'w') as out_file:\n",
    "#         json.dump(new_data, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Love this by Jim Whitehurst <mark>@RedHat</mark> &lt; \"Instead of being the commander-in-chief, the leader became the communicator-... https://t.co/gYcCe5NXvI', 'Using Keycloak instead of Picketlink for SAML-based authentication #RedHat https://t.co/XavswHIz9q', 'DevNation Live: Kubernetes enterprise integration patterns with Camel K #RedHat https://t.co/vSbE3Tgt1q', 'Die #RedHat Innovation Awards 2020 sind eroffnet. Nutzt ihr Open-Source-Losungen von Red Hat auf innovative Weise?... https://t.co/Q2MnnjOYMb', '#RedHat #OpenStack Platform remains a strategic piece of our #hybridcloud vision, bridging the gap between existing... https://t.co/qQ8VBT9OpI']\n"
     ]
    }
   ],
   "source": [
    "# DON'T RUN THIS\n",
    "# with open('new_posts.txt', encoding=\"utf-8\") as json_file:\n",
    "#     new_data = json.load(json_file)\n",
    "    \n",
    "# new_data = [result['title'] for result in new_data['results']]\n",
    "\n",
    "# print(new_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('Text', 'Label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(texts):\n",
    "    tw = []\n",
    "    pl = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = clean_sentence(text)\n",
    "        str(text)\n",
    "        sent  = TextBlob(text)\n",
    "        tw.append(text)\n",
    "        pl.append(sent.sentiment.polarity)\n",
    "\n",
    "    df['Text'] = tw\n",
    "    df['Label'] = pl\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29871, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_df(new_data)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>love whitehurst mark&gt;&lt;/mark instead commander ...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>keycloak instead picketlink saml base authenti...</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>devnation live kubernetes enterprise integrati...</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#redhat innovation awards 2020 sind eroffnet n...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#redhat #openstack platform remain strategic p...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Label\n",
       "0  love whitehurst mark></mark instead commander ...  0.500000\n",
       "1  keycloak instead picketlink saml base authenti... -0.800000\n",
       "2  devnation live kubernetes enterprise integrati...  0.136364\n",
       "3  #redhat innovation awards 2020 sind eroffnet n...  0.250000\n",
       "4  #redhat #openstack platform remain strategic p...  0.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_type(num):\n",
    "    if (num >= -1 and num < 0):\n",
    "        return 'NEGATIVE'\n",
    "    elif (num > 0 and num <= 1):\n",
    "        return 'POSITIVE'\n",
    "    else:\n",
    "        return 'NEUTRAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].apply(lambda x: sentiment_type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>love whitehurst mark&gt;&lt;/mark instead commander ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>keycloak instead picketlink saml base authenti...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>devnation live kubernetes enterprise integrati...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#redhat innovation awards 2020 sind eroffnet n...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#redhat #openstack platform remain strategic p...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Label\n",
       "0  love whitehurst mark></mark instead commander ...  POSITIVE\n",
       "1  keycloak instead picketlink saml base authenti...  NEGATIVE\n",
       "2  devnation live kubernetes enterprise integrati...  POSITIVE\n",
       "3  #redhat innovation awards 2020 sind eroffnet n...  POSITIVE\n",
       "4  #redhat #openstack platform remain strategic p...   NEUTRAL"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['Text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/large_tech.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/large_tech.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>love whitehurst mark&gt;&lt;/mark instead commander ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>keycloak instead picketlink saml base authenti...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>devnation live kubernetes enterprise integrati...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#redhat innovation awards 2020 sind eroffnet n...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#redhat #openstack platform remain strategic p...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Label\n",
       "0  love whitehurst mark></mark instead commander ...  POSITIVE\n",
       "1  keycloak instead picketlink saml base authenti...  NEGATIVE\n",
       "2  devnation live kubernetes enterprise integrati...  POSITIVE\n",
       "3  #redhat innovation awards 2020 sind eroffnet n...  POSITIVE\n",
       "4  #redhat #openstack platform remain strategic p...   NEUTRAL"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Text', 'Label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, output_dir=None, n_iter=20, n_texts=2000):\n",
    "    if model is not None:\n",
    "        # load existing spaCy model\n",
    "        nlp = spacy.load(model)\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        # Create blank Language class\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        print(\"Created new model\")\n",
    "\n",
    "    # Add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # Otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # Add label to text classifier\n",
    "    textcat.add_label('POSITIVE')\n",
    "    textcat.add_label('NEGATIVE')\n",
    "    textcat.add_label('NEUTRAL')\n",
    "\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "\n",
    "    # Get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    \n",
    "    # Only train textcat by disabling other pipes\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('No.', 'LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # Batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                print(losses)\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            \n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # Evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print('{0}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}'  # print a simple table\n",
    "                  .format(i, losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(limit=0, split=0.8):\n",
    "    # Partition off part of the train data for evaluation\n",
    "    cats = []\n",
    "    for y in df['Label']:\n",
    "        if (y == 'NEGATIVE'):\n",
    "            cats.append({ 'POSITIVE': 0, 'NEGATIVE': 1, 'NEUTRAL': 0})\n",
    "        elif (y == 'POSITIVE'):\n",
    "            cats.append({ 'POSITIVE': 1, 'NEGATIVE': 0, 'NEUTRAL': 0})\n",
    "        else:\n",
    "            cats.append({ 'POSITIVE': 0, 'NEGATIVE': 0, 'NEUTRAL': 1})\n",
    "\n",
    "    split = int(len(df['Text'].values) * split)\n",
    "    return (df['Text'][:split], cats[:split]), (df['Text'][split:], cats[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(model=None, output_dir=\"text_cnn_models/en_large_model\", n_iter=20, n_texts=19592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'text_cnn_models/en_large_model'\n",
    "\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "def predict_test(text):\n",
    "    test_text = clean_sentence(text)\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_classification_1",
   "language": "python",
   "name": "text_classification_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
