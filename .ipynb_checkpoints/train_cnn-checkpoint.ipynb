{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "from  spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from seaborn) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from seaborn) (1.17.1)\n",
      "Requirement already satisfied: pandas>=0.15.2 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from seaborn) (0.25.1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (2.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2019.2)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from cycler>=0.10->matplotlib>=1.4.3->seaborn) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from wordcloud) (1.17.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages (from wordcloud) (6.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation.replace(\"#\", \"\")\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whose', 'every', 'without', 'yourself', 'hereafter', '‘m', 'two', 'mostly', 'seem', 'therefore', \"'re\", 'alone', 'across', 'that', 'becomes', 'we', 'ten', 'anything', 'if', 'put', 'been', 'moreover', 'whom', 'would', 'still', 'elsewhere', 'many', 'per', 'them', 'while', 'own', \"n't\", 'thru', '’m', 'as', 'more', 'whereafter', '‘d', 'see', 'his', 'anywhere', 'off', 'get', 'name', 'until', 'enough', 'what', '‘ll', 're', 'full', 'former', 'whatever', 'but', 'toward', 'only', \"'s\", 'hereupon', 'whither', 'twelve', 'hereby', 'just', 'ca', 'somehow', 'do', 'front', 'my', 'various', 'less', 'who', 'anyhow', 'therein', 'well', \"'ve\", 'everywhere', 'yours', 'hence', 'those', 'amount', 'except', 'nothing', 'is', 'bottom', 'go', 'sometime', 'several', 'seems', 'whereby', 'cannot', 'three', 'nowhere', 'us', 'amongst', 'whole', 'itself', 'part', 'before', 'on', 'take', 'very', 'upon', 'keep', '’ll', 'anyway', 'another', 'against', \"'m\", 'once', 'should', 'thus', 'there', 'herself', 'below', 'one', 'whoever', 'why', 'done', 'anyone', 'ever', 'from', 'latter', 'again', 'are', 'be', 'because', 'besides', 'nor', 'further', 'now', 'often', 'or', 'become', 'side', 'together', 'can', 'everyone', 'third', 'yourselves', 'around', 'she', 'doing', 'each', 'an', 'already', 'after', 'ourselves', 'some', 'hundred', 'unless', 'most', 'becoming', 'your', 'within', 'then', 'empty', 'thereby', 'throughout', 'for', 'sometimes', 'to', 'our', 'has', 'none', 'somewhere', 'n‘t', 'were', 'between', 'no', '’ve', 'nevertheless', 'not', 'does', 'beyond', 'perhaps', 'something', 'please', 'all', 'used', 'had', 'indeed', 'make', 'everything', 'quite', 'am', 'could', 'noone', 'so', 'even', 'down', 'sixty', 'whence', 'made', 'wherever', 'move', 'which', '‘re', '‘ve', 'under', 'was', 'however', 'whenever', 'since', 'last', 'when', 'really', 'thereupon', 'n’t', 'onto', 'its', 'call', 'have', 'along', 'it', 'himself', 'same', 'these', 'over', 'into', 'though', 'next', 'due', 'seemed', 'via', 'by', \"'d\", 'never', 'nobody', 'here', 'serious', 'six', 'either', '’d', 'how', 'might', 'did', 'through', 'first', 'mine', 'out', 'whereupon', 'with', 'five', '’re', 'much', 'whether', 'the', 'yet', 'meanwhile', 'others', 'beforehand', 'and', 'afterwards', '’s', 'someone', 'else', 'than', 'must', 'me', 'in', 'of', 'eleven', 'whereas', 'themselves', 'became', 'such', 'back', 'any', 'forty', 'myself', 'eight', 'you', 'about', 'i', 'seeming', 'their', 'ours', 'both', 'beside', 'thence', 'top', 'few', 'towards', 'regarding', 'neither', 'behind', '‘s', 'hers', 'nine', 'almost', 'thereafter', 'formerly', 'he', 'during', 'a', 'herein', 'they', 'fifteen', 'too', 'show', 'among', 'wherein', 'will', 'other', 'twenty', 'least', \"'ll\", 'using', 'also', 'at', 'fifty', 'may', 'above', 'say', 'always', 'being', 'him', 'rather', 'otherwise', 'up', 'although', 'latterly', 'her', 'four', 'where', 'namely', 'this', 'give']\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    # Lower case all words and strip white spaces\n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "    # Remove all stop words and punctuations\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweet(tweet):\n",
    "    tokenized_tweet = [t.text for t in nlp(tweet)]\n",
    "    \n",
    "    tokenized_tweet = [stemmer.stem(t) for t in tokenized_tweet]\n",
    "    \n",
    "    tokenized_tweet = ' '.join(tokenized_tweet)\n",
    "    \n",
    "    return tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraseTerms = [\n",
    "        u\"windows 10\",\n",
    "        u\"big data\",\n",
    "        u\"data analytics\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'late logan daily #ad cloud analytics thanks'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Phrase matchers\n",
    "    phraseMatcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    phrasePatterns = [nlp.make_doc(text) for text in phraseTerms]\n",
    "    \n",
    "    phraseMatcher.add(\"Names\", None, *phrasePatterns)\n",
    "    \n",
    "    for match_id, start, end in phraseMatcher(doc):\n",
    "        span = doc[start:end]\n",
    "        sentence = sentence.replace(span.text, ''.join(token.text for token in span))\n",
    "    \n",
    "    # Token matchers\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # Create matcher to detect urls\n",
    "    pattern = [{ \"LIKE_URL\": True }]\n",
    "    matcher.add(\"UrlDetection\", None, pattern)\n",
    "    \n",
    "    # Create matcher to detect ...\n",
    "    pattern = [{ \"TEXT\": \"...\" }]\n",
    "    matcher.add(\"MoreDotsDetection\", None, pattern)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        sentence = sentence.replace(span.text, '')\n",
    "    \n",
    "    # Tokenize sentence and join\n",
    "    sentence = ' '.join(str(token) for token in spacy_tokenizer(sentence))\n",
    "    \n",
    "    # Remove twitter handles\n",
    "    sentence = remove_pattern(sentence, \"@[\\w]*\")\n",
    "    \n",
    "    sentence = '#'.join([phrase for phrase in [e[1:] for e in (' ' + sentence).split(\"#\")]])\n",
    "    \n",
    "    # remove words with length less than 3 and not #\n",
    "    sentence = ' '.join([word for word in sentence.split() if word[0] == '#' or len(word)>3])\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "clean_sentence(\"The latest Logan's DX Daily #AD (Cloud, AI/ML, Analytics & IoT)! https://t.co/Ac3cKz73Gx Thanks to @JD_Corporate... https://t.co/OOyzLIPxA2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>__label__Negative</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI #Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>__label__Negative</td>\n",
       "      <td>Data Analytics 'Performance Gap' Destroying Cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>__label__Negative</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI. - htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>__label__Negative</td>\n",
       "      <td>Big Data and the Problem of Bias in Higher Edu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>__label__Positive</td>\n",
       "      <td>Using Twitter for big data analytics to analyz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feedback                                               text\n",
       "0  __label__Negative  Big Data Is Dead. Long Live Big Data AI #Machi...\n",
       "1  __label__Negative  Data Analytics 'Performance Gap' Destroying Cu...\n",
       "2  __label__Negative  Big Data Is Dead. Long Live Big Data AI. - htt...\n",
       "3  __label__Negative  Big Data and the Problem of Bias in Higher Edu...\n",
       "4  __label__Positive  Using Twitter for big data analytics to analyz..."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/flair_labeled_sentiments_long.csv\", names=['feedback', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feedback'] = df['feedback'].replace('__label__Negative', 0)\n",
    "df['feedback'] = df['feedback'].replace('__label__Positive', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI #Machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analytics 'Performance Gap' Destroying Cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI. - htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data and the Problem of Bias in Higher Edu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Using Twitter for big data analytics to analyz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feedback                                               text\n",
       "0         0  Big Data Is Dead. Long Live Big Data AI #Machi...\n",
       "1         0  Data Analytics 'Performance Gap' Destroying Cu...\n",
       "2         0  Big Data Is Dead. Long Live Big Data AI. - htt...\n",
       "3         0  Big Data and the Problem of Bias in Higher Edu...\n",
       "4         1  Using Twitter for big data analytics to analyz..."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(2000)\n",
    "df['clean_text'] = np.vectorize(clean_sentence)(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI #Machi...</td>\n",
       "      <td>bigdata dead long live bigdata #machinelearnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analytics 'Performance Gap' Destroying Cu...</td>\n",
       "      <td>dataanalytics performance destroying customer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI. - htt...</td>\n",
       "      <td>bigdata dead long live bigdata #bigdata #dataa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data and the Problem of Bias in Higher Edu...</td>\n",
       "      <td>bigdata problem bias higher education #bigdata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Using Twitter for big data analytics to analyz...</td>\n",
       "      <td>twitter bigdataanalytics analyze disaster #soc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feedback                                               text  \\\n",
       "0         0  Big Data Is Dead. Long Live Big Data AI #Machi...   \n",
       "1         0  Data Analytics 'Performance Gap' Destroying Cu...   \n",
       "2         0  Big Data Is Dead. Long Live Big Data AI. - htt...   \n",
       "3         0  Big Data and the Problem of Bias in Higher Edu...   \n",
       "4         1  Using Twitter for big data analytics to analyz...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  bigdata dead long live bigdata #machinelearnin...  \n",
       "1  dataanalytics performance destroying customer ...  \n",
       "2  bigdata dead long live bigdata #bigdata #dataa...  \n",
       "3  bigdata problem bias higher education #bigdata...  \n",
       "4  twitter bigdataanalytics analyze disaster #soc...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['clean_text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI #Machi...</td>\n",
       "      <td>bigdata dead long live bigdata #machinelearnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analytics 'Performance Gap' Destroying Cu...</td>\n",
       "      <td>dataanalytics performance destroying customer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data Is Dead. Long Live Big Data AI. - htt...</td>\n",
       "      <td>bigdata dead long live bigdata #bigdata #dataa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Big Data and the Problem of Bias in Higher Edu...</td>\n",
       "      <td>bigdata problem bias higher education #bigdata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Using Twitter for big data analytics to analyz...</td>\n",
       "      <td>twitter bigdataanalytics analyze disaster #soc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Azure Big Data Analytics Platform Databricksto...</td>\n",
       "      <td>azure bigdataanalytics platform databrickstoka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>DZone &gt;&gt; Automate or Die: A Dramatic State of ...</td>\n",
       "      <td>dzone automate dramatic state affairs 21st cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Big data in a nutshell https://t.co/HxdnUe7IQI...</td>\n",
       "      <td>bigdata nutshell overview bigdata #crm fraud d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tired of your current job? Check this out! =&gt; ...</td>\n",
       "      <td>tired current check bigdata application develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Big Data And The Problem Of Bias In Higher Edu...</td>\n",
       "      <td>bigdata problem bias higher education explosiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feedback                                               text  \\\n",
       "0          0  Big Data Is Dead. Long Live Big Data AI #Machi...   \n",
       "1          0  Data Analytics 'Performance Gap' Destroying Cu...   \n",
       "2          0  Big Data Is Dead. Long Live Big Data AI. - htt...   \n",
       "3          0  Big Data and the Problem of Bias in Higher Edu...   \n",
       "4          1  Using Twitter for big data analytics to analyz...   \n",
       "7          1  Azure Big Data Analytics Platform Databricksto...   \n",
       "8          1  DZone >> Automate or Die: A Dramatic State of ...   \n",
       "9          1  Big data in a nutshell https://t.co/HxdnUe7IQI...   \n",
       "10         1  Tired of your current job? Check this out! => ...   \n",
       "11         1  Big Data And The Problem Of Bias In Higher Edu...   \n",
       "\n",
       "                                           clean_text  \n",
       "0   bigdata dead long live bigdata #machinelearnin...  \n",
       "1   dataanalytics performance destroying customer ...  \n",
       "2   bigdata dead long live bigdata #bigdata #dataa...  \n",
       "3   bigdata problem bias higher education #bigdata...  \n",
       "4   twitter bigdataanalytics analyze disaster #soc...  \n",
       "7   azure bigdataanalytics platform databrickstoka...  \n",
       "8   dzone automate dramatic state affairs 21st cen...  \n",
       "9   bigdata nutshell overview bigdata #crm fraud d...  \n",
       "10  tired current check bigdata application develo...  \n",
       "11  bigdata problem bias higher education explosiv...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, output_dir=None, n_iter=20, n_texts=2000):\n",
    "    if model is not None:\n",
    "        # load existing spaCy model\n",
    "        nlp = spacy.load(model)\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        # Create blank Language class\n",
    "        nlp = spacy.blank('en')\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # Add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # Otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # Add label to text classifier\n",
    "    textcat.add_label('POSITIVE')\n",
    "\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "\n",
    "    # Get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    \n",
    "    # Only train textcat by disabling other pipes\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('No.', 'LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # Batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            \n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # Evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print('{0}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}'  # print a simple table\n",
    "                  .format(i, losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(limit=0, split=0.8):\n",
    "    # Partition off part of the train data for evaluation\n",
    "    cats = [{'POSITIVE': bool(y)} for y in df['feedback']]\n",
    "    split = int(len(df['clean_text'].values) * split)\n",
    "    return (df['clean_text'][:split], cats[:split]), (df['clean_text'][split:], cats[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Using 1894 examples (1499 training, 375 evaluation)\n",
      "Training the model...\n",
      " No. \\LOSS \t  P  \t  R  \t  F  \n",
      "0\t4.506\t1.000\t1.000\t1.000\n",
      "1\t4.053\t1.000\t1.000\t1.000\n",
      "2\t3.255\t1.000\t0.989\t0.995\n",
      "3\t2.333\t1.000\t0.984\t0.992\n",
      "4\t1.788\t1.000\t0.965\t0.982\n",
      "5\t1.238\t1.000\t0.944\t0.971\n",
      "6\t1.088\t1.000\t0.931\t0.964\n",
      "7\t0.725\t1.000\t0.925\t0.961\n",
      "8\t0.637\t1.000\t0.925\t0.961\n",
      "9\t0.566\t1.000\t0.941\t0.970\n",
      "10\t0.479\t1.000\t0.931\t0.964\n",
      "11\t0.603\t1.000\t0.944\t0.971\n",
      "12\t0.492\t1.000\t0.947\t0.973\n",
      "13\t0.371\t1.000\t0.947\t0.973\n",
      "14\t0.563\t1.000\t0.949\t0.974\n",
      "15\t0.257\t1.000\t0.936\t0.967\n",
      "16\t0.353\t1.000\t0.928\t0.963\n",
      "17\t0.379\t1.000\t0.931\t0.964\n",
      "18\t0.308\t1.000\t0.928\t0.963\n",
      "19\t0.167\t1.000\t0.931\t0.964\n",
      "Saved model to text_cnn_models\\model\n"
     ]
    }
   ],
   "source": [
    "main(model=None, output_dir=\"text_cnn_models/model\", n_iter=20, n_texts=1892)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'text_cnn_models/model'\n",
    "\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "def predict_test(text):\n",
    "    test_text = clean_sentence(text)\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate work #windows miss current #redhat .... {'POSITIVE': 0.9962859153747559}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"This is why I always hated working with #Windows. No, I was not missing it at all in my current job in #RedHat.... https://t.co/1gZlVbPwHH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makeuseof hate wireless {'POSITIVE': 0.9977158308029175}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"RT MakeUseOf: hate using it on your wireless ro... https://t.co/JLuKKhUC96\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want 50000 dogecoins free register click #crypto #bitcoin #giveaway {'POSITIVE': 0.9994334578514099}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"Want to win 50000 DOGECOINS Free? Register with only 2 clicks! https://t.co/CgpuxohQDe - #crypto #bitcoin #giveaway... https://t.co/ieTGdXvI2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter #win bruntmor enameled 12-inch deep saute color #follow #retweet winner announced september {'POSITIVE': 0.9791330695152283}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"RT @bruntmor: Enter to #win Bruntmor's Enameled 12-Inch Deep Saute Pan in Any Color! #follow #retweet Winner Announced September 5th, USA O...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dead #windows #mac #linux headup 2020nian niyan saremashita {'POSITIVE': 0.9976792931556702}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"Dead End Job #Windows #Mac #Linux <Headup> ga, 2020Nian Pei Xin niYan Qi saremashita. https://t.co/d8Auyfps9F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gears meet funko available download android windows10 playe {'POSITIVE': 0.9999101161956787}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"RT @OriginalFunko: Gears of War meets Funko is available for download now on iOS, Android and Windows 10 PC! Have you playe...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows10 update #microsoft change iconic feature happy #operatingsystem ...... {'POSITIVE': 0.9956941604614258}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"Windows 10 update: #Microsoft could change this iconic feature and some fans aren't happy #operatingsystem...... https://t.co/wuquSEakzH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suse linux understand destroy windows10 partition process {'POSITIVE': 0.024774516001343727}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"@vzverovich SuSE linux 6.1. I didn't understand anything about it and destroyed my Windows10 partition in the process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freeze windows10 {'POSITIVE': 0.9970779418945312}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"freezing windows 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redhatsupport choosing right datum storage complicate learn block storage file storage {'POSITIVE': 0.0010454102884978056}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"RT RedHatSupport: Choosing the right data storage is complicated. Learn more about block storage, file storage, and... https://t.co/zJOVbQh5sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maga symbol hate racism brand company trump {'POSITIVE': 0.12466123700141907}\n"
     ]
    }
   ],
   "source": [
    "predict_test(\"@RedHat Red Hat = MAGA = A symbol of hate & racism. Bad branding. Unless your company is pro-Trump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_classification_1",
   "language": "python",
   "name": "text_classification_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
