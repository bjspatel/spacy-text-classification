{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_lg\n",
    "from  spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "punctuations = string.punctuation.replace(\"#\", \"\")\n",
    "parser = English()\n",
    "nlp = en_core_web_lg.load()\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    # Lower case all words and strip white spaces\n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "    # Remove all stop words and punctuations\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'late logan mark marktag</mark daily #ad cloud analytics thanks'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Token matchers\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # Create matcher to detect urls\n",
    "    pattern = [{ \"LIKE_URL\": True }]\n",
    "    matcher.add(\"UrlDetection\", None, pattern)\n",
    "    \n",
    "    # Create matcher to detect ...\n",
    "    pattern = [{ \"TEXT\": \"...\" }]\n",
    "    matcher.add(\"MoreDotsDetection\", None, pattern)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        sentence = sentence.replace(span.text, '')\n",
    "    \n",
    "    # Tokenize sentence and join\n",
    "    sentence = ' '.join(str(token) for token in spacy_tokenizer(sentence))\n",
    "    \n",
    "    # Remove twitter handles\n",
    "    sentence = remove_pattern(sentence, \"@[\\w]*\")\n",
    "    \n",
    "    sentence = '#'.join([phrase for phrase in [e[1:] for e in (' ' + sentence).split(\"#\")]])\n",
    "\n",
    "    # remove words with length less than 3 and not #\n",
    "    sentence = ' '.join([word for word in sentence.split() if word[0] == '#' or len(word)>3])\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "clean_sentence(\"The latest Logan's <mark>marktag</mark> DX Daily #AD (Cloud, AI/ML, Analytics & IoT)! https://t.co/Ac3cKz73Gx Thanks to @JD_Corporate... https://t.co/OOyzLIPxA2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29871\n"
     ]
    }
   ],
   "source": [
    "# DON'T RUN THIS\n",
    "# new_data = { 'results': [] }\n",
    "\n",
    "# with open('new_posts.txt', encoding=\"utf-8\") as json_file:\n",
    "#     new_data = json.load(json_file)\n",
    "\n",
    "# with open('posts.txt', encoding=\"utf-8\") as read_file:\n",
    "#     data = json.load(read_file)\n",
    "#     new_data['results'] = new_data['results'] + data['results']\n",
    "    \n",
    "#     print(len(new_data['results']))\n",
    "#     with open('new_posts.txt', 'w') as out_file:\n",
    "#         json.dump(new_data, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Love this by Jim Whitehurst <mark>@RedHat</mark> &lt; \"Instead of being the commander-in-chief, the leader became the communicator-... https://t.co/gYcCe5NXvI', 'Using Keycloak instead of Picketlink for SAML-based authentication #RedHat https://t.co/XavswHIz9q', 'DevNation Live: Kubernetes enterprise integration patterns with Camel K #RedHat https://t.co/vSbE3Tgt1q', 'Die #RedHat Innovation Awards 2020 sind eroffnet. Nutzt ihr Open-Source-Losungen von Red Hat auf innovative Weise?... https://t.co/Q2MnnjOYMb', '#RedHat #OpenStack Platform remains a strategic piece of our #hybridcloud vision, bridging the gap between existing... https://t.co/qQ8VBT9OpI']\n"
     ]
    }
   ],
   "source": [
    "# DON'T RUN THIS\n",
    "# with open('new_posts.txt', encoding=\"utf-8\") as json_file:\n",
    "#     new_data = json.load(json_file)\n",
    "    \n",
    "# new_data = [result['title'] for result in new_data['results']]\n",
    "\n",
    "# print(new_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('Text', 'Label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(texts):\n",
    "    tw = []\n",
    "    pl = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = clean_sentence(text)\n",
    "        str(text)\n",
    "        sent  = TextBlob(text)\n",
    "        tw.append(text)\n",
    "        pl.append(sent.sentiment.polarity)\n",
    "\n",
    "    df['Text'] = tw\n",
    "    df['Label'] = pl\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29871, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_df(new_data)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>love whitehurst mark&gt;&lt;/mark instead commander ...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>keycloak instead picketlink saml base authenti...</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>devnation live kubernetes enterprise integrati...</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#redhat innovation awards 2020 sind eroffnet n...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#redhat #openstack platform remain strategic p...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Label\n",
       "0  love whitehurst mark></mark instead commander ...  0.500000\n",
       "1  keycloak instead picketlink saml base authenti... -0.800000\n",
       "2  devnation live kubernetes enterprise integrati...  0.136364\n",
       "3  #redhat innovation awards 2020 sind eroffnet n...  0.250000\n",
       "4  #redhat #openstack platform remain strategic p...  0.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_type(num):\n",
    "    if (num >= -1 and num < 0):\n",
    "        return 'NEGATIVE'\n",
    "    elif (num > 0 and num <= 1):\n",
    "        return 'POSITIVE'\n",
    "    else:\n",
    "        return 'NEUTRAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].apply(lambda x: sentiment_type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>love whitehurst mark&gt;&lt;/mark instead commander ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>keycloak instead picketlink saml base authenti...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>devnation live kubernetes enterprise integrati...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#redhat innovation awards 2020 sind eroffnet n...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#redhat #openstack platform remain strategic p...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Label\n",
       "0  love whitehurst mark></mark instead commander ...  POSITIVE\n",
       "1  keycloak instead picketlink saml base authenti...  NEGATIVE\n",
       "2  devnation live kubernetes enterprise integrati...  POSITIVE\n",
       "3  #redhat innovation awards 2020 sind eroffnet n...  POSITIVE\n",
       "4  #redhat #openstack platform remain strategic p...   NEUTRAL"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['Text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/large_tech.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/large_tech.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>love whitehurst mark&gt;&lt;/mark instead commander ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>keycloak instead picketlink saml base authenti...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>devnation live kubernetes enterprise integrati...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#redhat innovation awards 2020 sind eroffnet n...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#redhat #openstack platform remain strategic p...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Label\n",
       "0  love whitehurst mark></mark instead commander ...  POSITIVE\n",
       "1  keycloak instead picketlink saml base authenti...  NEGATIVE\n",
       "2  devnation live kubernetes enterprise integrati...  POSITIVE\n",
       "3  #redhat innovation awards 2020 sind eroffnet n...  POSITIVE\n",
       "4  #redhat #openstack platform remain strategic p...   NEUTRAL"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Text', 'Label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, output_dir=None, n_iter=20, n_texts=2000):\n",
    "    if model is not None:\n",
    "        # load existing spaCy model\n",
    "        nlp = spacy.load(model)\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        # Create blank Language class\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        print(\"Created new model\")\n",
    "\n",
    "    # Add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # Otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # Add label to text classifier\n",
    "    textcat.add_label('POSITIVE')\n",
    "    textcat.add_label('NEGATIVE')\n",
    "    textcat.add_label('NEUTRAL')\n",
    "\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "    print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "          .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "    train_data = list(zip(train_texts,\n",
    "                          [{'cats': cats} for cats in train_cats]))\n",
    "\n",
    "    # Get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    \n",
    "    # Only train textcat by disabling other pipes\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('No.', 'LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # Batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                print(losses)\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            \n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # Evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print('{0}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}'  # print a simple table\n",
    "                  .format(i, losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(limit=0, split=0.8):\n",
    "    # Partition off part of the train data for evaluation\n",
    "    cats = []\n",
    "    for y in df['Label']:\n",
    "        if (y == 'NEGATIVE'):\n",
    "            cats.append({ 'POSITIVE': 0, 'NEGATIVE': 1, 'NEUTRAL': 0})\n",
    "        elif (y == 'POSITIVE'):\n",
    "            cats.append({ 'POSITIVE': 1, 'NEGATIVE': 0, 'NEUTRAL': 0})\n",
    "        else:\n",
    "            cats.append({ 'POSITIVE': 0, 'NEGATIVE': 0, 'NEUTRAL': 1})\n",
    "\n",
    "#     cats = [{'POSITIVE': bool(y == 'POSITIVE'), 'NEGATIVE': bool(y == 'NEGATIVE')} for y in df['Label']]\n",
    "            \n",
    "    split = int(len(df['Text'].values) * split)\n",
    "    return (df['Text'][:split], cats[:split]), (df['Text'][split:], cats[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model\n",
      "Using 19592 examples (19194 training, 4799 evaluation)\n",
      "Training the model...\n",
      " No. \tLOSS \t  P  \t  R  \t  F  \n",
      "{}\n",
      "{'textcat': 0.07284726202487946}\n",
      "{'textcat': 0.13863155245780945}\n",
      "{'textcat': 0.22739459574222565}\n",
      "{'textcat': 0.3002947121858597}\n",
      "{'textcat': 0.3907225579023361}\n",
      "{'textcat': 0.47457844763994217}\n",
      "{'textcat': 0.5526265427470207}\n",
      "{'textcat': 0.6497661098837852}\n",
      "{'textcat': 0.7438048794865608}\n",
      "{'textcat': 0.7873785011470318}\n",
      "{'textcat': 0.8647123537957668}\n",
      "{'textcat': 0.901713952422142}\n",
      "{'textcat': 0.9673654139041901}\n",
      "{'textcat': 1.040707990527153}\n",
      "{'textcat': 1.116337925195694}\n",
      "{'textcat': 1.2122694998979568}\n",
      "{'textcat': 1.267979759722948}\n",
      "{'textcat': 1.3247257433831692}\n",
      "{'textcat': 1.3909948281943798}\n",
      "{'textcat': 1.4694473557174206}\n",
      "{'textcat': 1.5159569196403027}\n",
      "{'textcat': 1.5627251528203487}\n",
      "{'textcat': 1.605339601635933}\n",
      "{'textcat': 1.654737152159214}\n",
      "{'textcat': 1.7047277018427849}\n",
      "{'textcat': 1.7524692043662071}\n",
      "{'textcat': 1.808828167617321}\n",
      "{'textcat': 1.866751603782177}\n",
      "{'textcat': 1.92067451775074}\n",
      "{'textcat': 1.98422522097826}\n",
      "{'textcat': 2.0545517951250076}\n",
      "{'textcat': 2.113372251391411}\n",
      "{'textcat': 2.184873402118683}\n",
      "{'textcat': 2.2470660284161568}\n",
      "{'textcat': 2.31515484303236}\n",
      "{'textcat': 2.3776836544275284}\n",
      "{'textcat': 2.4305738359689713}\n",
      "{'textcat': 2.4931667670607567}\n",
      "{'textcat': 2.55504934117198}\n",
      "{'textcat': 2.6149007380008698}\n",
      "{'textcat': 2.6709707975387573}\n",
      "{'textcat': 2.731283325701952}\n",
      "{'textcat': 2.7937013134360313}\n",
      "{'textcat': 2.862575553357601}\n",
      "{'textcat': 2.9180503748357296}\n",
      "{'textcat': 2.959138624370098}\n",
      "{'textcat': 3.0290743336081505}\n",
      "{'textcat': 3.0784688517451286}\n",
      "{'textcat': 3.1320579797029495}\n",
      "{'textcat': 3.184499841183424}\n",
      "{'textcat': 3.2389619946479797}\n",
      "{'textcat': 3.2966991662979126}\n",
      "{'textcat': 3.3543650209903717}\n",
      "{'textcat': 3.397174868732691}\n",
      "{'textcat': 3.455439291894436}\n",
      "{'textcat': 3.4974562749266624}\n",
      "{'textcat': 3.576914928853512}\n",
      "{'textcat': 3.6249626763164997}\n",
      "{'textcat': 3.6794804371893406}\n",
      "{'textcat': 3.7489010132849216}\n",
      "{'textcat': 3.7881852723658085}\n",
      "{'textcat': 3.8358692340552807}\n",
      "{'textcat': 3.914022695273161}\n",
      "{'textcat': 3.9741712249815464}\n",
      "{'textcat': 4.012675628066063}\n",
      "{'textcat': 4.074546724557877}\n",
      "{'textcat': 4.138480290770531}\n",
      "{'textcat': 4.220040708780289}\n",
      "{'textcat': 4.29775857925415}\n",
      "{'textcat': 4.346541352570057}\n",
      "{'textcat': 4.409767173230648}\n",
      "{'textcat': 4.45825657248497}\n",
      "{'textcat': 4.513872276991606}\n",
      "{'textcat': 4.555655151605606}\n",
      "{'textcat': 4.6197992488741875}\n",
      "{'textcat': 4.653621327131987}\n",
      "{'textcat': 4.692598033696413}\n",
      "{'textcat': 4.745340719819069}\n",
      "{'textcat': 4.797770757228136}\n",
      "{'textcat': 4.858882151544094}\n",
      "{'textcat': 4.888230226933956}\n",
      "{'textcat': 4.961737729609013}\n",
      "{'textcat': 5.0201254189014435}\n",
      "{'textcat': 5.07260812073946}\n",
      "{'textcat': 5.12692116945982}\n",
      "{'textcat': 5.177273720502853}\n",
      "{'textcat': 5.22838543727994}\n",
      "{'textcat': 5.265329301357269}\n",
      "{'textcat': 5.319381259381771}\n",
      "{'textcat': 5.362513706088066}\n",
      "{'textcat': 5.41887691617012}\n",
      "{'textcat': 5.457199055701494}\n",
      "{'textcat': 5.508215427398682}\n",
      "{'textcat': 5.5622559785842896}\n",
      "{'textcat': 5.6232118010520935}\n",
      "{'textcat': 5.678351283073425}\n",
      "{'textcat': 5.731279291212559}\n",
      "{'textcat': 5.787087347358465}\n",
      "{'textcat': 5.827887732535601}\n",
      "{'textcat': 5.8881429098546505}\n",
      "{'textcat': 5.941969905048609}\n",
      "{'textcat': 5.989311654120684}\n",
      "{'textcat': 6.033500738441944}\n",
      "{'textcat': 6.0952029675245285}\n",
      "{'textcat': 6.146790690720081}\n",
      "{'textcat': 6.201746121048927}\n",
      "{'textcat': 6.248554609715939}\n",
      "{'textcat': 6.283807717263699}\n",
      "{'textcat': 6.344912625849247}\n",
      "{'textcat': 6.391259387135506}\n",
      "{'textcat': 6.436612073332071}\n",
      "{'textcat': 6.484002698212862}\n",
      "{'textcat': 6.536859631538391}\n",
      "{'textcat': 6.588564567267895}\n",
      "{'textcat': 6.625725753605366}\n",
      "{'textcat': 6.655568525195122}\n",
      "{'textcat': 6.732364758849144}\n",
      "{'textcat': 6.793230704963207}\n",
      "{'textcat': 6.831203974783421}\n",
      "{'textcat': 6.899392180144787}\n",
      "{'textcat': 6.964781627058983}\n",
      "{'textcat': 7.012484349310398}\n",
      "{'textcat': 7.050941146910191}\n",
      "{'textcat': 7.1011594496667385}\n",
      "{'textcat': 7.152141407132149}\n",
      "{'textcat': 7.209326632320881}\n",
      "{'textcat': 7.259207874536514}\n",
      "{'textcat': 7.305559992790222}\n",
      "{'textcat': 7.351907409727573}\n",
      "{'textcat': 7.397830106317997}\n",
      "{'textcat': 7.442610263824463}\n",
      "{'textcat': 7.476715333759785}\n",
      "{'textcat': 7.51276358217001}\n",
      "{'textcat': 7.576447509229183}\n",
      "{'textcat': 7.61648740619421}\n",
      "{'textcat': 7.667082577943802}\n",
      "{'textcat': 7.709148213267326}\n",
      "{'textcat': 7.7658533826470375}\n",
      "{'textcat': 7.827395644038916}\n",
      "{'textcat': 7.877558447420597}\n",
      "{'textcat': 7.9165591187775135}\n",
      "{'textcat': 7.967832732945681}\n",
      "{'textcat': 8.00953408330679}\n",
      "{'textcat': 8.055104326456785}\n",
      "{'textcat': 8.101499114185572}\n",
      "{'textcat': 8.152634501457214}\n",
      "{'textcat': 8.194445431232452}\n",
      "{'textcat': 8.250721283257008}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-ff11673ffaa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text_cnn_models/en_large_model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_texts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m19592\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-04d60f5b8d64>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(model, output_dir, n_iter, n_texts)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n\u001b[1;32m---> 47\u001b[1;33m                            losses=losses)\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtextcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\documents\\spacy_text_classification_1\\spacy_env\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE151\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munexpected_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munexpected_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m                 \u001b[0mgold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGoldParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m             \u001b[0mdoc_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m             \u001b[0mgold_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mgold.pyx\u001b[0m in \u001b[0;36mspacy.gold.GoldParse.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "main(model=None, output_dir=\"text_cnn_models/en_large_model\", n_iter=20, n_texts=19592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'text_cnn_models/en_large_model'\n",
    "\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "def predict_test(text):\n",
    "    test_text = clean_sentence(text)\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_classification_1",
   "language": "python",
   "name": "text_classification_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
